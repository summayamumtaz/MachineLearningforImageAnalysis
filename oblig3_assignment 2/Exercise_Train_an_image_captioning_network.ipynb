{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Train an image captioning network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"*Image Captioning is the process of generating textual description of an image. It uses both Natural Language Processing and Computer Vision to generate the captions*\"\n",
    "\n",
    "In this mandaroty exercise you are implementing an image captioning network. The network will consist of an encoder and a decoder. The encoder is a convolutional neural network, and the decoder is a recurrent neural network. Producing reasonable textual description of an image is a hard task, however with the use of a CNN and a RNN we can start to generate somewhat plausible descriptions. \n",
    "\n",
    "\n",
    "Links:\n",
    "- [Task1: Implement useful functions](#Task1)\n",
    "- [Task2: Train the recurrent neural network](#Task2)\n",
    "- [Task3: Generate image captions](#Task3)\n",
    "\n",
    "\n",
    "Software version:\n",
    "- Python 3.6\n",
    "- TensorFlow 1.4.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluation format ###\n",
    "\n",
    "\n",
    "You will be guided through the implementation step by step, and you can check your implementation at each step. Note, you will often need to complete all previous steps in order to continue.\n",
    "\n",
    "In the implementation part, test functions allow you to check your code rapidly. when your code works, you can apply it on part 2 to train and part 3 to generate captions. Part 2, the training process, is slow and can run over night on a typical laptop computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise content\n",
    "\n",
    "\n",
    "All subtasks that you are to answer are found in this notebook. All implementation should be done in the file \"RNN_network.py\" found in folder \"/sourceFiles/\". The skeleton of the program is already implemented and contains things such as:\n",
    "- Importing data\n",
    "- How the tensors are connected within the computational graph\n",
    "- Training framework\n",
    "\n",
    "\n",
    "As mentioned, an image captioning network consists of an encoder and a decoder. Your task is to implement the decoder (RNN). The images have already been processed through a CNN. The feature vectors are stored in pickle files together with corresponding labels.\n",
    "\n",
    "\n",
    "During task 1, you will implement all required functionalities for training the image captioning network. In task 2, you will train the network and study how different RNN arcitectures influences the loss. You will generate image captions from images in the validation set in task 3. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dataset ###\n",
    "\n",
    "We will use a dataset called \"Common Object in Context\" (COCO) 2017. It has ~120,000 training and 5,000 validation images. Every image also includes ~5 captions.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"utils_images/bear.png\",width=300>\n",
    "*Figure 1:*\n",
    "\n",
    "Captions:\n",
    "\n",
    "- A big burly grizzly bear is show with grass in the background.\n",
    "- The large brown bear has a black nose.\n",
    "- Closeup of a brown bear sitting in a grassy area.\n",
    "- A large bear that is sitting on grass. \n",
    "- A close up picture of a brown bear's face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Network architecture ###\n",
    "\n",
    "**Encoder**\n",
    "\n",
    "\n",
    "Convolutional neural networks have shown to be useful for extracting high level features from images. We will use a pretrained VGG16 network trained on ImageNet. ImageNet consists of 1,2 million images distributed over 1000 classes, and we hope the model have learnt many general features. We are not interested in classifying the 1000 classes, and will change the last fully connected layer with our own. We will use a tanh actiation function to squeeze the values between -1 and 1 similar to the recurrent cells. \n",
    "\n",
    "**Decoder**\n",
    "\n",
    "To be able to convert the high level features from the encoder to natural language, we will construct a recurrent neural network. The output of the encoder will be passed as the initial state to the recurrent cells. The input to the recurret neural network will be word embeddings which we will learn. \n",
    "\n",
    "**Loss function**\n",
    "\n",
    "The words will be considered as separate classes and we shall use cross entropy loss.\n",
    "\n",
    "**Training vs testing**\n",
    "\n",
    "When we train the RNN, we will feed in the correct token (word) for every time step, see figure 2a. The words (tokens) are generally unknown, and in test mode, we will need to use our best estimate as input. Example, if the word \"brown\" has the highest probabillity after the softmax at timestep 2, we will feed in \"brown\" as input at timestep 3, see figure 2b.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"utils_images/image_captioning_diagram_train.png\",width=2200>\n",
    "\n",
    "<br>\n",
    "*Figure 2a:* The figure shows an example with 3 recurrent cells stacked in training mode. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"utils_images/image_captioning_diagram_test.png\",width=2200>\n",
    "\n",
    "<br>\n",
    "*Figure 2b:* The figure shows an example with 3 recurrent cells stacked in test mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vocabulary ###\n",
    "\n",
    "\n",
    "To make good decisions in terms of the network architecture, we will need to know the statistics of the captions:\n",
    "- The number of words in the captions (sequence length) should be considered when chooseing the truncated backpropagation length. \n",
    "- To save memory, it is normal to limit the vocabulary size/length. There will be a tradeoff between capturing all words and have a reansonble sized softmax layer. \n",
    "\n",
    "\n",
    "Note: The captions have been filtered such that all special characters have been removed. This includes also punctuations and commas. All characters have been changed to lower case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "**Vocabulary size**\n",
    "\n",
    "To make good predictions we need the model to have the abillity to predict frequent words. Figure 3 shows a sorted histogram of the word count for the different words. We can see that the majority of the words are within the 2000 most frequent words. Figure 4 shows the precentage of the words accounted for as a function of the vocabulary size.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"utils_images/Word_count_hist.png\", width=800>\n",
    "*Figure 3*\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"utils_images/accumulated_Word_count.png\",width=800>\n",
    "*Figure 4*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Truncated backpropagation length**\n",
    "\n",
    "\n",
    "The sequence length of the recurrent neural network should be able to capture the time(step) dependencies within the captions. Figure 5 shows a histogram of the caption counts as a function of the caption (/sequence) length. Figure 6 shows the precentage of all captions with shorter or equal caption length as a function of caption (/sequence) length. \n",
    "\n",
    "<br>\n",
    "<img src=\"utils_images/Sequence_lengths_hist.png\", width=800>\n",
    "*Figure 5*\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"utils_images/accumulated_Sequence_length_count.png\", width=800>\n",
    "*Figure 6*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Common words and their tokens**\n",
    "\n",
    "\n",
    "Every word is assosiated with a token. The words are sorted such that the most frequent words have lower token values. In the table below, you find the six most common words in the dataset. The three first words/tokens have a spescial purpuse:\n",
    "- \"eeee\": Indicates the end of a caption.\n",
    "- \"ssss\": Start token, first word \n",
    "- \"UNK\": As the vocabulary size is can be smaller than the number of words in the datasset, we change all \"unknown\" words to the word \"UNK\"\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"utils_images/vocabulary.png\", width=800>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "### Word embedding  ###\n",
    "\n",
    "\n",
    "The input to the first rnn layer is the embedded version of the input words. We will define a word embedding matrix with shape [vocabulary_size, embedding_size)]. A single word embedding will be a row vector with length [embedding_size]. The tokens are used to select the correct row within the word embedding matrix. For example, the word \"a\" will have values in the third row in the embedding matrix (were first row is row zero).\n",
    "\n",
    "\n",
    "a_emb = wordEmbedding[a_token, :]\n",
    "\n",
    "\n",
    "The word embedding matrix will be initialized with random values and they will be updated as part of the training process. The goal is that similar words get similar vector representations within the embedding matrix. This will not be part of the assigment, but as a test, the vector representations could be embedded using e.g. t-SNE for plotting in 2D/3D space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Data preparation  ###\n",
    "\n",
    "Before starting to work on implementing the network aritecture, you should check if you need to download and process data:\n",
    "\n",
    "- **UIO**: If you work on a UIO IFI computer, we have already processed and made the data avaiable for you on the project disk. The path to this disk is given as default in the code. \n",
    "\n",
    "- **Personal computer**: If you plan to work on the assigment on your personal computer, you will need to download and process the data yourself. Follow the steps in notebook \"data_preparation.ipynb\". Running the data preparation can take several hours due to the size of this real data set. \n",
    "\n",
    "\n",
    "The processed data is pickle files holding the following information for every image:\n",
    "- Path to the jpg image\n",
    "- Captions\n",
    "- Captions given as tokens\n",
    "- The fc7 output from the VGG16 network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Task1'></a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Task 1: Implement useful functions #\n",
    "See function headings for specification of each function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "**Task 1a): getInputPlaceholders()** \n",
    "\n",
    "To be able to feed data into the computational graph, we can use tf.placeholders. Your task is to implement the function \"getInputPlaceholders()\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import tensorflow as tf\n",
    "from sourceFiles import RNN_network \n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Resetting the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Defining dummy variables\n",
    "VggFc7Size = 4096\n",
    "truncated_backprop_length = 20\n",
    "\n",
    "# You should implement this function\n",
    "xVggFc7, xTokens = RNN_network.getInputPlaceholders(VggFc7Size, truncated_backprop_length)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.checkGetInputPlaceholders(xVggFc7, xTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "**Task 1b): getInitialState()** \n",
    "\n",
    "We will pass the output layer (fc7) from the VGG16 network through a fully connected layer before using it as the initial state of the recurrent cells. Your task is to implement the fully connected layer, using tanh-activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import tensorflow as tf\n",
    "from sourceFiles import RNN_network \n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Resetting the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Defining dummy variables\n",
    "VggFc7Size = 4096\n",
    "xVggFc7 = tf.random_normal(shape=(256, 4096), name='xVggFc7', seed=0)\n",
    "hidden_state_sizes = 512\n",
    "\n",
    "# You should implement this function\n",
    "initial_state = RNN_network.getInitialState(xVggFc7, VggFc7Size, hidden_state_sizes)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.checkGetInitialState(initial_state, hidden_state_sizes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1c** \n",
    "\n",
    "Each word will be represented by a feature vector. The feature vectors are rows in the wordEmbeddingMatrix. Complete getWordEmbeddingMatrix to set up the embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import tensorflow as tf\n",
    "from sourceFiles import RNN_network \n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Resetting the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Defining dummy variables\n",
    "vocabulary_size = 5000\n",
    "embedding_size  = 128\n",
    "\n",
    "# You should implement this function\n",
    "wordEmbeddingMatrix = RNN_network.getWordEmbeddingMatrix(vocabulary_size, embedding_size)\n",
    "\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.checkgetWordEmbeddingMatrix(wordEmbeddingMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1d** \n",
    "\n",
    "Implement the function \"getInputs()\".\n",
    "Hint: tf.nn.embedding_lookup and tf.unstack might be useful.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import tensorflow as tf\n",
    "from sourceFiles import RNN_network\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Resetting the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Defining dummy variables\n",
    "wordEmbeddingMatrix = tf.random_uniform(shape=(1000, 256))\n",
    "xTokens             = tf.random_uniform(shape=(128, 8), minval=0, maxval=999, dtype=tf.int32)\n",
    "\n",
    "\n",
    "# You should implement this function\n",
    "inputs = RNN_network.getInputs(wordEmbeddingMatrix, xTokens)\n",
    "\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.checkGetInputs(inputs, wordEmbeddingMatrix, xTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1e** \n",
    "\n",
    "After the last RNN cell, we feed the features from the hidden state into a fully connected layer. Your task is to implement the function, getRNNOutputWeights(), which returns the weights to be used in the fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import tensorflow as tf\n",
    "from sourceFiles import RNN_network\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Resetting the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Defining dummy variables\n",
    "hidden_state_sizes = 128\n",
    "vocabulary_size    = 1000\n",
    "\n",
    "\n",
    "# You should implement this function\n",
    "W_hy, b_hy = RNN_network.getRNNOutputWeights(hidden_state_sizes, vocabulary_size)\n",
    "\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.checkGetRNNOutputWeights(W_hy, b_hy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1f** \n",
    "\n",
    "Your are to implement a vanailla RNN class. The class shall have a constructor and the function \"forward\". To speed up inference, we will concatinate the input and the old state matrices and perform a single matrix multiplication. The difference is illustrated in the equation below.\n",
    "\n",
    "Vanilla RNN:\n",
    "\n",
    "\\begin{align}\n",
    "h_t &= tanh(x_tW^{hx} + h_{t-1}W^{hh} + b) \\\\\n",
    "\\\\\n",
    "h_t &= tanh([x_t,h_{t-1}]W + b)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import tensorflow as tf\n",
    "from sourceFiles import RNN_network\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Resetting the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Defining dummy variables\n",
    "hidden_state_sizes = 512\n",
    "inputSize = 648\n",
    "ind = 0\n",
    "\n",
    "# You should implement this function\n",
    "cell   = RNN_network.RNNcell(hidden_state_sizes, inputSize, ind)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.checkRNNcell(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1g** \n",
    "\n",
    "Your are to implement a Gated recurrent units (GRU) class. The class shall have a constructor and the function \"forward\". To speed up inference, we will concatinate the input and the old state matrices and perform a single matrix multiplication. The difference is illustrated in the equations below, were you are to implement the equations in the column to the right.\n",
    "\n",
    "GRU:\n",
    "\n",
    "\\begin{align}\n",
    "&Update \\: gate: \\qquad &\\Gamma^u=\\sigma(x_tW^{u} + h_{t-1}U^{u} + b^u) \\qquad \\rightarrow \\qquad &\\Gamma^u=\\sigma([x_t, h_{t-1}]W^{u} + b^u) \\\\\n",
    "&Reset \\: gate: \\qquad &\\Gamma^r=\\sigma(x_tW^{r} + h_{t-1}U^{r} + b^r) \\qquad \\rightarrow \\qquad &\\Gamma^r=\\sigma([x_t, h_{t-1}]W^{r} + b^r) \\\\\n",
    "&Candidate \\:cell: &\\tilde{h_t} = tanh([x_tW + (\\Gamma^r \\circ h_{t-1})U + b) \\qquad \\rightarrow \\qquad &\\tilde{h_t} = tanh([x_t, (\\Gamma^r \\circ h_{t-1})] W + b) \\\\\n",
    "&Final\\: cell:    &h_t = \\Gamma^u \\circ h_{t-1} + (1-\\Gamma^u) \\circ \\tilde{h_t} \\qquad \\rightarrow \\qquad &h_t = \\Gamma^u \\circ h_{t-1} + (1-\\Gamma^u) \\circ \\tilde{h_t}\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import tensorflow as tf\n",
    "from sourceFiles import RNN_network\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Resetting the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Defining dummy variables\n",
    "hidden_state_sizes = 512\n",
    "inputSize = 648\n",
    "ind = 0\n",
    "\n",
    "# You should implement this function\n",
    "cell   = RNN_network.GRUcell(hidden_state_sizes, inputSize, ind)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.checkGRUcell(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1h** \n",
    "\n",
    "You are now to implement the function \"buildRNN\". Use the \"is_training\" flag to choose between training and testing mode. Tensorflow can be slow with dynamic routing, so you are to build different static graphs for training and testing. See figure 2a and 2b. The two cells below will test your implementation in training and testing mode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import tensorflow as tf\n",
    "from sourceFiles import RNN_network\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Resetting the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Config\n",
    "cellType            = 'GRU'   # of RNN or GRU\n",
    "is_training         = True    #True/False\n",
    "\n",
    "networkConfig, inputs, initial_states, wordEmbedding, W_hy, b_hy, batch_size = unit_tests.getConfig(cellType)\n",
    "\n",
    "# You should implement this function\n",
    "logits_series, predictions_series, current_state, predicted_tokens = RNN_network.buildRNN(networkConfig, inputs, initial_states, wordEmbedding, W_hy, b_hy, is_training)\n",
    "\n",
    "\n",
    "tests = ['check_current_state', 'check_logits_series', 'predictions_series']\n",
    "unit_tests.checkRNNbuild(networkConfig, tests, is_training, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import tensorflow as tf\n",
    "from sourceFiles import RNN_network\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Resetting the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Config\n",
    "cellType            = 'GRU'   # of RNN or GRU\n",
    "is_training         = False    #True/False\n",
    "\n",
    "networkConfig, inputs, initial_states, wordEmbedding, W_hy, b_hy, batch_size = unit_tests.getConfig(cellType)\n",
    "\n",
    "# You should implement this function\n",
    "logits_series, predictions_series, current_state, predicted_tokens = RNN_network.buildRNN(networkConfig, inputs, initial_states, wordEmbedding, W_hy, b_hy, is_training)\n",
    "\n",
    "\n",
    "tests = ['predicted_tokens']\n",
    "unit_tests.checkRNNbuild(networkConfig, tests, is_training, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Task2'></a>\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "# Task 2: Train the image captioning network #\n",
    "\n",
    "\n",
    "Congratulations, you are done with all implemention. You shall now train various network architectures. The weights will be stored regularly, but updated when the validation loss has decrease only. When you are done training you can go to task 3 to start generating captions. \n",
    "\n",
    "Depending on your compute power, the training can take a long time. Start with small simple network architectures.\n",
    "\n",
    "\n",
    "Tips: Do not use GradientDescent optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_coco\n",
    "import tensorflow as tf\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#training config\n",
    "trainingConfig   = {}\n",
    "trainingConfig['trainBatchSize'] = 256\n",
    "trainingConfig['valBatchSize']   = 256\n",
    "trainingConfig['learning_rate']  = 0.001\n",
    "trainingConfig['numberOfEpochs'] = 10\n",
    "trainingConfig['getLossOnValSetEvery#epoch'] = 0.2\n",
    "trainingConfig['saveModelEvery#epoch']       = 1\n",
    "trainingConfig['optimizerType']              = 'RMSProp'   #Adam, RMSProp, GradientDescent\n",
    "trainingConfig['inNotebook'] = True\n",
    "\n",
    "\n",
    "#Network config\n",
    "networkConfig = {}\n",
    "networkConfig['VggFc7Size']      = 4096 #Fixed, do not change\n",
    "networkConfig['embedding_size']  = 128\n",
    "networkConfig['vocabulary_size'] = 4000\n",
    "networkConfig['truncated_backprop_length'] = 20\n",
    "networkConfig['hidden_state_sizes'] = 128\n",
    "networkConfig['num_layers']         = 1\n",
    "networkConfig['is_training']        = True  #\n",
    "networkConfig['cellType']           = 'RNN' #RNN or GRU\n",
    "\n",
    "#Create an instance of the \"trainClass\"\n",
    "myTrainClass = train_coco.trainClass(trainingConfig, networkConfig)\n",
    "\n",
    "#Path if you work on personal computer\n",
    "#path = 'data/coco/'\n",
    "\n",
    "#Path if you work on UIO IFI computer\n",
    "path = /projects/anne/p22/inf5860data/\n",
    "\n",
    "myTrainClass.myCocoDataClass.setDatapath(path)\n",
    "\n",
    "myTrainClass.myCocoDataClass.getfilePaths()\n",
    "\n",
    "myTrainClass.buildGraph()\n",
    "\n",
    "myTrainClass.loss()\n",
    "\n",
    "myTrainClass.optimizer()\n",
    "\n",
    "myTrainClass.tensorboard()\n",
    "\n",
    "myTrainClass.configureSession()\n",
    "\n",
    "myTrainClass.saveWeights()\n",
    "\n",
    "myTrainClass.trainingLoop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Task3'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Task 3: Generate image captions on the validation set#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_coco\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "#training config\n",
    "trainingConfig   = {}\n",
    "trainingConfig['trainBatchSize'] = 256\n",
    "trainingConfig['valBatchSize']   = 1\n",
    "trainingConfig['learning_rate']  = 0.001\n",
    "trainingConfig['numberOfEpochs'] = 10\n",
    "trainingConfig['getLossOnValSetEvery#epoch'] = 0.2\n",
    "trainingConfig['saveModelEvery#epoch']       = 1\n",
    "trainingConfig['optimizerType']              = 'RMSProp'   #Adam, RMSProp, GradientDescent\n",
    "trainingConfig['inNotebook'] = True\n",
    "\n",
    "\n",
    "#Network config\n",
    "networkConfig = {}\n",
    "networkConfig['VggFc7Size']      = 4096 #Fixed, do not change\n",
    "networkConfig['embedding_size']  = 128\n",
    "networkConfig['vocabulary_size'] = 4000\n",
    "networkConfig['truncated_backprop_length'] = 20\n",
    "networkConfig['hidden_state_sizes'] = 128\n",
    "networkConfig['num_layers']         = 1\n",
    "networkConfig['is_training']        = False  #\n",
    "networkConfig['cellType']           = 'RNN' #RNN or GRU\n",
    "\n",
    "myTrainClass = train_coco.trainClass(trainingConfig, networkConfig)\n",
    "\n",
    "#Path if you work on personal computer\n",
    "#path = 'data/coco/'\n",
    "\n",
    "#Path if you work on UIO IFI computer\n",
    "path = /projects/anne/p22/inf5860data/\n",
    "\n",
    "myTrainClass.myCocoDataClass.setDatapath(path)\n",
    "\n",
    "myTrainClass.myCocoDataClass.getfilePaths()\n",
    "\n",
    "myTrainClass.buildGraph()\n",
    "\n",
    "myTrainClass.configureSession()\n",
    "\n",
    "myTrainClass.restoreWeights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the cell below to generate captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTrainClass.plotImagesAndCaptions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
